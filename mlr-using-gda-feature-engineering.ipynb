{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1423636,"sourceType":"datasetVersion","datasetId":833557}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Compresive Strenght of Concrete Using Machine Learning\nCompressive strength is the most widely recognized and accepted measure of concrete's strength. It serves as the primary criterion for assessing whether a specific concrete mixture can endure the structural loads it will encounter. Often referred to as the \"nameplate\" rating, compressive strength is the most frequently cited attribute in construction specifications.\n\nTo test compressive strength, cylindrical concrete samples are broken in a specialized machine, following the ASTM (American Society for Testing & Materials) standard C39 protocol.\n\nCompressive strength of concrete is measured in pounds per square inch (psi), representing the force in pounds applied over a square inch area, or in megapascals (MPa) for metric units. This measurement is obtained by applying force from opposite sides of the concrete sample until it fractures, revealing the strength limits of the cured mix. Aggregate materials within the concrete help distribute and counterbalance the load. Generally, a higher psi indicates greater compressive strength and typically comes with a higher cost, but it also implies enhanced durability, longevity, and often a more efficient use of material compared to lower-strength mixes.\n\nCompressive strength is normally tested at seven days and then again at 28 days. The seven-day test determines early strength gains and verifies that the mix is on track to set properly. The final cured design strength (and the basis for minimum design values) is the 28-day test as noted in the ACI standards.","metadata":{}},{"cell_type":"markdown","source":"In this project,  we will use the following features: \n- **Cement**: kg in a m³ mixture\n- **Blast Furnace Slag**: kg in a m³ mixture\n- **Fly Ash**: kg in a m³ mixture\n- **Water**: kg in a m³ mixture\n- **Superplasticizer**: kg in a m³ mixture\n- **Coarse Aggregate**: kg in a m³ mixture\n- **Fine Aggregate**: kg in a m³ mixture\n- **Age**: Day (1~365)\n\nto predict the target variable: \n- **Concrete Compressive Strength**: MPa\n\n","metadata":{}},{"cell_type":"markdown","source":"For this project we will use Mean Squared Error (MSE) for both cost function and evaluation metrics. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\n\ndf = pd.read_csv('/kaggle/input/regression-with-neural-networking/concrete_data.csv')\ndf2 = pd.read_csv('/kaggle/input/regression-with-neural-networking/concrete_data.csv')\ndf3 = pd.read_csv('/kaggle/input/regression-with-neural-networking/concrete_data.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.330466Z","iopub.execute_input":"2024-06-22T05:39:46.331146Z","iopub.status.idle":"2024-06-22T05:39:46.347424Z","shell.execute_reply.started":"2024-06-22T05:39:46.331101Z","shell.execute_reply":"2024-06-22T05:39:46.346211Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.589185Z","iopub.execute_input":"2024-06-22T05:39:46.590042Z","iopub.status.idle":"2024-06-22T05:39:46.609541Z","shell.execute_reply.started":"2024-06-22T05:39:46.590009Z","shell.execute_reply":"2024-06-22T05:39:46.608603Z"},"trusted":true},"execution_count":141,"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"      Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0      540.0                 0.0      0.0  162.0               2.5   \n1      540.0                 0.0      0.0  162.0               2.5   \n2      332.5               142.5      0.0  228.0               0.0   \n3      332.5               142.5      0.0  228.0               0.0   \n4      198.6               132.4      0.0  192.0               0.0   \n...      ...                 ...      ...    ...               ...   \n1025   276.4               116.0     90.3  179.6               8.9   \n1026   322.2                 0.0    115.6  196.0              10.4   \n1027   148.5               139.4    108.6  192.7               6.1   \n1028   159.1               186.7      0.0  175.6              11.3   \n1029   260.9               100.5     78.3  200.6               8.6   \n\n      Coarse Aggregate  Fine Aggregate  Age  Strength  \n0               1040.0           676.0   28     79.99  \n1               1055.0           676.0   28     61.89  \n2                932.0           594.0  270     40.27  \n3                932.0           594.0  365     41.05  \n4                978.4           825.5  360     44.30  \n...                ...             ...  ...       ...  \n1025             870.1           768.3   28     44.28  \n1026             817.9           813.4   28     31.18  \n1027             892.4           780.0   28     23.70  \n1028             989.6           788.9   28     32.77  \n1029             864.5           761.5   28     32.40  \n\n[1030 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1025</th>\n      <td>276.4</td>\n      <td>116.0</td>\n      <td>90.3</td>\n      <td>179.6</td>\n      <td>8.9</td>\n      <td>870.1</td>\n      <td>768.3</td>\n      <td>28</td>\n      <td>44.28</td>\n    </tr>\n    <tr>\n      <th>1026</th>\n      <td>322.2</td>\n      <td>0.0</td>\n      <td>115.6</td>\n      <td>196.0</td>\n      <td>10.4</td>\n      <td>817.9</td>\n      <td>813.4</td>\n      <td>28</td>\n      <td>31.18</td>\n    </tr>\n    <tr>\n      <th>1027</th>\n      <td>148.5</td>\n      <td>139.4</td>\n      <td>108.6</td>\n      <td>192.7</td>\n      <td>6.1</td>\n      <td>892.4</td>\n      <td>780.0</td>\n      <td>28</td>\n      <td>23.70</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>159.1</td>\n      <td>186.7</td>\n      <td>0.0</td>\n      <td>175.6</td>\n      <td>11.3</td>\n      <td>989.6</td>\n      <td>788.9</td>\n      <td>28</td>\n      <td>32.77</td>\n    </tr>\n    <tr>\n      <th>1029</th>\n      <td>260.9</td>\n      <td>100.5</td>\n      <td>78.3</td>\n      <td>200.6</td>\n      <td>8.6</td>\n      <td>864.5</td>\n      <td>761.5</td>\n      <td>28</td>\n      <td>32.40</td>\n    </tr>\n  </tbody>\n</table>\n<p>1030 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"By printing the information about the dataframe, we have foundthat there is no missing values and each variables has the correct datatype. ","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.610922Z","iopub.execute_input":"2024-06-22T05:39:46.611211Z","iopub.status.idle":"2024-06-22T05:39:46.620323Z","shell.execute_reply.started":"2024-06-22T05:39:46.611187Z","shell.execute_reply":"2024-06-22T05:39:46.619384Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1030 entries, 0 to 1029\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   Cement              1030 non-null   float64\n 1   Blast Furnace Slag  1030 non-null   float64\n 2   Fly Ash             1030 non-null   float64\n 3   Water               1030 non-null   float64\n 4   Superplasticizer    1030 non-null   float64\n 5   Coarse Aggregate    1030 non-null   float64\n 6   Fine Aggregate      1030 non-null   float64\n 7   Age                 1030 non-null   int64  \n 8   Strength            1030 non-null   float64\ndtypes: float64(8), int64(1)\nmemory usage: 72.5 KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Regression using SGDREgressor Class\nSGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).\n\nWe will use the SGDRegressor class from sklearn.linear_model package.Prior to this, we need to scale the features since the SGDRegressor class is sensitive to the scale of the features. Large difference in features scales can lead to numerical instability during gradient computation. ","metadata":{}},{"cell_type":"markdown","source":"#### Extracting the Features & Splitting","metadata":{}},{"cell_type":"code","source":"#extract the explanatory and response variable\n#axis = 1 indicates that we are droping a column\n#axis = 1 indicates that we are droping from row\nX = df.drop('Strength', axis = 1)\nY = df['Strength']\n\n#split into train and test. use 80-20 rule\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.621918Z","iopub.execute_input":"2024-06-22T05:39:46.622197Z","iopub.status.idle":"2024-06-22T05:39:46.631597Z","shell.execute_reply.started":"2024-06-22T05:39:46.622174Z","shell.execute_reply":"2024-06-22T05:39:46.630662Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"markdown","source":"#### Training\nIn this part, the loss='squared_error' argument indicates that the cost function to be used is MSE, the learning rate (alpha) is set to 0.0001, and the tolerance is e^-3. This indicates that convergence is achieved if the cost function decreases by <= tolerance. The model does not necessarily need to repeat 1000 times if it converges sooner. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#instance\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\n\n#instance of the model and training\nmodel = SGDRegressor(loss='squared_error', alpha=0.00001, max_iter=1000, tol=1e-3, random_state=42, verbose = 1)\nmodel.fit(X_train_scaled, Y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.863807Z","iopub.execute_input":"2024-06-22T05:39:46.864524Z","iopub.status.idle":"2024-06-22T05:39:46.886626Z","shell.execute_reply.started":"2024-06-22T05:39:46.864486Z","shell.execute_reply":"2024-06-22T05:39:46.885683Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"-- Epoch 1\nNorm: 10.97, NNZs: 8, Bias: 31.470546, T: 824, Avg. loss: 190.203997\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 12.10, NNZs: 8, Bias: 34.636910, T: 1648, Avg. loss: 63.275791\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 12.64, NNZs: 8, Bias: 35.385086, T: 2472, Avg. loss: 59.210603\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 12.91, NNZs: 8, Bias: 35.794916, T: 3296, Avg. loss: 58.434260\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 13.16, NNZs: 8, Bias: 35.821443, T: 4120, Avg. loss: 58.079001\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 13.41, NNZs: 8, Bias: 35.739610, T: 4944, Avg. loss: 57.734515\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 13.65, NNZs: 8, Bias: 35.853215, T: 5768, Avg. loss: 57.457101\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 13.77, NNZs: 8, Bias: 35.760282, T: 6592, Avg. loss: 57.472421\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 13.84, NNZs: 8, Bias: 35.770327, T: 7416, Avg. loss: 57.330591\nTotal training time: 0.00 seconds.\n-- Epoch 10\nNorm: 14.05, NNZs: 8, Bias: 35.786275, T: 8240, Avg. loss: 57.114527\nTotal training time: 0.00 seconds.\n-- Epoch 11\nNorm: 14.19, NNZs: 8, Bias: 35.834274, T: 9064, Avg. loss: 56.914654\nTotal training time: 0.00 seconds.\n-- Epoch 12\nNorm: 14.32, NNZs: 8, Bias: 35.820192, T: 9888, Avg. loss: 57.044334\nTotal training time: 0.00 seconds.\n-- Epoch 13\nNorm: 14.39, NNZs: 8, Bias: 35.824609, T: 10712, Avg. loss: 56.965339\nTotal training time: 0.00 seconds.\n-- Epoch 14\nNorm: 14.44, NNZs: 8, Bias: 35.817006, T: 11536, Avg. loss: 56.897059\nTotal training time: 0.00 seconds.\n-- Epoch 15\nNorm: 14.52, NNZs: 8, Bias: 35.809341, T: 12360, Avg. loss: 56.814003\nTotal training time: 0.00 seconds.\n-- Epoch 16\nNorm: 14.58, NNZs: 8, Bias: 35.814950, T: 13184, Avg. loss: 56.737775\nTotal training time: 0.00 seconds.\n-- Epoch 17\nNorm: 14.71, NNZs: 8, Bias: 35.907250, T: 14008, Avg. loss: 56.698715\nTotal training time: 0.00 seconds.\n-- Epoch 18\nNorm: 14.83, NNZs: 8, Bias: 35.919000, T: 14832, Avg. loss: 56.601159\nTotal training time: 0.00 seconds.\n-- Epoch 19\nNorm: 14.87, NNZs: 8, Bias: 35.776048, T: 15656, Avg. loss: 56.599984\nTotal training time: 0.00 seconds.\n-- Epoch 20\nNorm: 14.96, NNZs: 8, Bias: 35.781023, T: 16480, Avg. loss: 56.540853\nTotal training time: 0.00 seconds.\n-- Epoch 21\nNorm: 15.02, NNZs: 8, Bias: 35.779024, T: 17304, Avg. loss: 56.567685\nTotal training time: 0.00 seconds.\n-- Epoch 22\nNorm: 15.04, NNZs: 8, Bias: 35.819644, T: 18128, Avg. loss: 56.508012\nTotal training time: 0.00 seconds.\n-- Epoch 23\nNorm: 15.15, NNZs: 8, Bias: 35.871705, T: 18952, Avg. loss: 56.434435\nTotal training time: 0.00 seconds.\n-- Epoch 24\nNorm: 15.21, NNZs: 8, Bias: 35.877863, T: 19776, Avg. loss: 56.304862\nTotal training time: 0.00 seconds.\n-- Epoch 25\nNorm: 15.24, NNZs: 8, Bias: 35.928142, T: 20600, Avg. loss: 56.392548\nTotal training time: 0.00 seconds.\n-- Epoch 26\nNorm: 15.31, NNZs: 8, Bias: 35.869875, T: 21424, Avg. loss: 56.377969\nTotal training time: 0.00 seconds.\n-- Epoch 27\nNorm: 15.34, NNZs: 8, Bias: 35.826998, T: 22248, Avg. loss: 56.303861\nTotal training time: 0.00 seconds.\n-- Epoch 28\nNorm: 15.36, NNZs: 8, Bias: 35.860795, T: 23072, Avg. loss: 56.326516\nTotal training time: 0.00 seconds.\n-- Epoch 29\nNorm: 15.44, NNZs: 8, Bias: 35.873273, T: 23896, Avg. loss: 56.304934\nTotal training time: 0.00 seconds.\n-- Epoch 30\nNorm: 15.42, NNZs: 8, Bias: 35.816174, T: 24720, Avg. loss: 56.168171\nTotal training time: 0.00 seconds.\n-- Epoch 31\nNorm: 15.55, NNZs: 8, Bias: 35.859291, T: 25544, Avg. loss: 56.269792\nTotal training time: 0.00 seconds.\n-- Epoch 32\nNorm: 15.61, NNZs: 8, Bias: 35.871299, T: 26368, Avg. loss: 56.211523\nTotal training time: 0.00 seconds.\n-- Epoch 33\nNorm: 15.68, NNZs: 8, Bias: 35.861920, T: 27192, Avg. loss: 56.214544\nTotal training time: 0.00 seconds.\n-- Epoch 34\nNorm: 15.75, NNZs: 8, Bias: 35.842660, T: 28016, Avg. loss: 56.176208\nTotal training time: 0.00 seconds.\n-- Epoch 35\nNorm: 15.77, NNZs: 8, Bias: 35.873371, T: 28840, Avg. loss: 56.168405\nTotal training time: 0.00 seconds.\nConvergence after 35 epochs took 0.00 seconds\n","output_type":"stream"},{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"SGDRegressor(alpha=1e-05, random_state=42, verbose=1)","text/html":"<style>#sk-container-id-23 {color: black;background-color: white;}#sk-container-id-23 pre{padding: 0;}#sk-container-id-23 div.sk-toggleable {background-color: white;}#sk-container-id-23 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-23 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-23 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-23 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-23 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-23 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-23 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-23 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-23 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-23 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-23 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-23 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-23 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-23 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-23 div.sk-item {position: relative;z-index: 1;}#sk-container-id-23 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-23 div.sk-item::before, #sk-container-id-23 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-23 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-23 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-23 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-23 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-23 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-23 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-23 div.sk-label-container {text-align: center;}#sk-container-id-23 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-23 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-23\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(alpha=1e-05, random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" checked><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(alpha=1e-05, random_state=42, verbose=1)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Evaluation\n","metadata":{}},{"cell_type":"code","source":"#evaluating the model's performance\nY_pred = model.predict(X_test_scaled)\nmse = mean_squared_error(Y_test, Y_pred)\nprint(f\"Mean squared error on testing set: {mse}\")\n\nmpe = np.mean((Y_pred - Y_test) / Y_test) * 100\nprint(f\"Mean percentage error on testing set: {mpe} %\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.888206Z","iopub.execute_input":"2024-06-22T05:39:46.888513Z","iopub.status.idle":"2024-06-22T05:39:46.894798Z","shell.execute_reply.started":"2024-06-22T05:39:46.888487Z","shell.execute_reply":"2024-06-22T05:39:46.893853Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Mean squared error on testing set: 97.48045906978886\nMean percentage error on testing set: 13.323840623231206 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model converges after 35th epoch (iteration). From our evaluation metrics, the MSE is 97.480, which is equivalent to 13.324% in Mean Percentage Error (MPE). MPE focuses on the percentage difference between predicted and actual values. We can improve this model by including engineered features. ","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering - Manually\n\nFeature engineering is the process of selecting, manipulating and transforming raw data (or features) into features that can be used in supervised learning. This can involved tranforming the existing feature (e.g., taking the log) or combining features. ","metadata":{}},{"cell_type":"code","source":"\ndf['Aggregate Content'] = df['Coarse Aggregate'] + df['Fine Aggregate']\n\n# Total Binder Content\ndf['Total Binder Content'] = df['Cement'] + df['Blast Furnace Slag'] + df['Fly Ash']\n\n# Water to Cement Ratio (W/C Ratio)\ndf['Water to Cement Ratio'] = df['Water'] / df['Cement']\n\n# Water to Binder Ratio (W/B Ratio)\ndf['Water to Binder Ratio'] = df['Water'] / df['Total Binder Content']\n\n# Superplasticizer to Binder Ratio\ndf['Superplasticizer to Binder Ratio'] = df['Superplasticizer'] / df['Total Binder Content']\n\n# Aggregate Cggregate Ratio\ndf['Fine to Total Aggregate Ratio'] = df['Fine Aggregate'] / df['Aggregate Content']\n\n# Binder to Aggregate Ratio\ndf['Binder to Aggregate Ratio'] = df['Total Binder Content'] / df['Aggregate Content']\n\n# Cement Proportion in Binder\ndf['Cement Proportion in Binder'] = df['Cement'] / df['Total Binder Content']\n\n# Slag Proportion in Binder\ndf['Slag Proportion in Binder'] = df['Blast Furnace Slag'] / df['Total Binder Content']\n\n# Fly Ash Proportion in Binder\ndf['Fly Ash Proportion in Binder'] = df['Fly Ash'] / df['Total Binder Content']\n\n# Water Content per Unit Binder\ndf['Water Content per Unit Binder'] = df['Water'] / df['Total Binder Content']\n\n# Superplasticizer Content per Unit Binder\ndf['Superplasticizer Content per Unit Binder'] = df['Superplasticizer'] / df['Total Binder Content']\n\n# Total Aggregate to Cement Ratio\ndf['Total Aggregate to Cement Ratio'] = df['Aggregate Content'] / df['Cement']\n\n# Age Squared\ndf['Age Squared'] = df['Age'] ** 2\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.895878Z","iopub.execute_input":"2024-06-22T05:39:46.896149Z","iopub.status.idle":"2024-06-22T05:39:46.913066Z","shell.execute_reply.started":"2024-06-22T05:39:46.896126Z","shell.execute_reply":"2024-06-22T05:39:46.912194Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.914981Z","iopub.execute_input":"2024-06-22T05:39:46.915269Z","iopub.status.idle":"2024-06-22T05:39:46.951419Z","shell.execute_reply.started":"2024-06-22T05:39:46.915241Z","shell.execute_reply":"2024-06-22T05:39:46.950515Z"},"trusted":true},"execution_count":147,"outputs":[{"execution_count":147,"output_type":"execute_result","data":{"text/plain":"      Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0      540.0                 0.0      0.0  162.0               2.5   \n1      540.0                 0.0      0.0  162.0               2.5   \n2      332.5               142.5      0.0  228.0               0.0   \n3      332.5               142.5      0.0  228.0               0.0   \n4      198.6               132.4      0.0  192.0               0.0   \n...      ...                 ...      ...    ...               ...   \n1025   276.4               116.0     90.3  179.6               8.9   \n1026   322.2                 0.0    115.6  196.0              10.4   \n1027   148.5               139.4    108.6  192.7               6.1   \n1028   159.1               186.7      0.0  175.6              11.3   \n1029   260.9               100.5     78.3  200.6               8.6   \n\n      Coarse Aggregate  Fine Aggregate  Age  Strength  Aggregate Content  ...  \\\n0               1040.0           676.0   28     79.99             1716.0  ...   \n1               1055.0           676.0   28     61.89             1731.0  ...   \n2                932.0           594.0  270     40.27             1526.0  ...   \n3                932.0           594.0  365     41.05             1526.0  ...   \n4                978.4           825.5  360     44.30             1803.9  ...   \n...                ...             ...  ...       ...                ...  ...   \n1025             870.1           768.3   28     44.28             1638.4  ...   \n1026             817.9           813.4   28     31.18             1631.3  ...   \n1027             892.4           780.0   28     23.70             1672.4  ...   \n1028             989.6           788.9   28     32.77             1778.5  ...   \n1029             864.5           761.5   28     32.40             1626.0  ...   \n\n      Superplasticizer to Binder Ratio  Fine to Total Aggregate Ratio  \\\n0                             0.004630                       0.393939   \n1                             0.004630                       0.390526   \n2                             0.000000                       0.389253   \n3                             0.000000                       0.389253   \n4                             0.000000                       0.457620   \n...                                ...                            ...   \n1025                          0.018438                       0.468933   \n1026                          0.023755                       0.498621   \n1027                          0.015385                       0.466396   \n1028                          0.032678                       0.443576   \n1029                          0.019559                       0.468327   \n\n      Binder to Aggregate Ratio  Cement Proportion in Binder  \\\n0                      0.314685                     1.000000   \n1                      0.311958                     1.000000   \n2                      0.311271                     0.700000   \n3                      0.311271                     0.700000   \n4                      0.183491                     0.600000   \n...                         ...                          ...   \n1025                   0.294617                     0.572612   \n1026                   0.268375                     0.735952   \n1027                   0.237084                     0.374527   \n1028                   0.194434                     0.460093   \n1029                   0.270418                     0.593359   \n\n      Slag Proportion in Binder  Fly Ash Proportion in Binder  \\\n0                      0.000000                      0.000000   \n1                      0.000000                      0.000000   \n2                      0.300000                      0.000000   \n3                      0.300000                      0.000000   \n4                      0.400000                      0.000000   \n...                         ...                           ...   \n1025                   0.240315                      0.187073   \n1026                   0.000000                      0.264048   \n1027                   0.351576                      0.273897   \n1028                   0.539907                      0.000000   \n1029                   0.228565                      0.178076   \n\n      Water Content per Unit Binder  Superplasticizer Content per Unit Binder  \\\n0                          0.300000                                  0.004630   \n1                          0.300000                                  0.004630   \n2                          0.480000                                  0.000000   \n3                          0.480000                                  0.000000   \n4                          0.580060                                  0.000000   \n...                             ...                                       ...   \n1025                       0.372074                                  0.018438   \n1026                       0.447693                                  0.023755   \n1027                       0.486003                                  0.015385   \n1028                       0.507808                                  0.032678   \n1029                       0.456220                                  0.019559   \n\n      Total Aggregate to Cement Ratio  Age Squared  \n0                            3.177778          784  \n1                            3.205556          784  \n2                            4.589474        72900  \n3                            4.589474       133225  \n4                            9.083082       129600  \n...                               ...          ...  \n1025                         5.927641          784  \n1026                         5.063004          784  \n1027                        11.261953          784  \n1028                        11.178504          784  \n1029                         6.232273          784  \n\n[1030 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n      <th>Aggregate Content</th>\n      <th>...</th>\n      <th>Superplasticizer to Binder Ratio</th>\n      <th>Fine to Total Aggregate Ratio</th>\n      <th>Binder to Aggregate Ratio</th>\n      <th>Cement Proportion in Binder</th>\n      <th>Slag Proportion in Binder</th>\n      <th>Fly Ash Proportion in Binder</th>\n      <th>Water Content per Unit Binder</th>\n      <th>Superplasticizer Content per Unit Binder</th>\n      <th>Total Aggregate to Cement Ratio</th>\n      <th>Age Squared</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n      <td>1716.0</td>\n      <td>...</td>\n      <td>0.004630</td>\n      <td>0.393939</td>\n      <td>0.314685</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.300000</td>\n      <td>0.004630</td>\n      <td>3.177778</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n      <td>1731.0</td>\n      <td>...</td>\n      <td>0.004630</td>\n      <td>0.390526</td>\n      <td>0.311958</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.300000</td>\n      <td>0.004630</td>\n      <td>3.205556</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n      <td>1526.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.389253</td>\n      <td>0.311271</td>\n      <td>0.700000</td>\n      <td>0.300000</td>\n      <td>0.000000</td>\n      <td>0.480000</td>\n      <td>0.000000</td>\n      <td>4.589474</td>\n      <td>72900</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n      <td>1526.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.389253</td>\n      <td>0.311271</td>\n      <td>0.700000</td>\n      <td>0.300000</td>\n      <td>0.000000</td>\n      <td>0.480000</td>\n      <td>0.000000</td>\n      <td>4.589474</td>\n      <td>133225</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n      <td>1803.9</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.457620</td>\n      <td>0.183491</td>\n      <td>0.600000</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.580060</td>\n      <td>0.000000</td>\n      <td>9.083082</td>\n      <td>129600</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1025</th>\n      <td>276.4</td>\n      <td>116.0</td>\n      <td>90.3</td>\n      <td>179.6</td>\n      <td>8.9</td>\n      <td>870.1</td>\n      <td>768.3</td>\n      <td>28</td>\n      <td>44.28</td>\n      <td>1638.4</td>\n      <td>...</td>\n      <td>0.018438</td>\n      <td>0.468933</td>\n      <td>0.294617</td>\n      <td>0.572612</td>\n      <td>0.240315</td>\n      <td>0.187073</td>\n      <td>0.372074</td>\n      <td>0.018438</td>\n      <td>5.927641</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>1026</th>\n      <td>322.2</td>\n      <td>0.0</td>\n      <td>115.6</td>\n      <td>196.0</td>\n      <td>10.4</td>\n      <td>817.9</td>\n      <td>813.4</td>\n      <td>28</td>\n      <td>31.18</td>\n      <td>1631.3</td>\n      <td>...</td>\n      <td>0.023755</td>\n      <td>0.498621</td>\n      <td>0.268375</td>\n      <td>0.735952</td>\n      <td>0.000000</td>\n      <td>0.264048</td>\n      <td>0.447693</td>\n      <td>0.023755</td>\n      <td>5.063004</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>1027</th>\n      <td>148.5</td>\n      <td>139.4</td>\n      <td>108.6</td>\n      <td>192.7</td>\n      <td>6.1</td>\n      <td>892.4</td>\n      <td>780.0</td>\n      <td>28</td>\n      <td>23.70</td>\n      <td>1672.4</td>\n      <td>...</td>\n      <td>0.015385</td>\n      <td>0.466396</td>\n      <td>0.237084</td>\n      <td>0.374527</td>\n      <td>0.351576</td>\n      <td>0.273897</td>\n      <td>0.486003</td>\n      <td>0.015385</td>\n      <td>11.261953</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>159.1</td>\n      <td>186.7</td>\n      <td>0.0</td>\n      <td>175.6</td>\n      <td>11.3</td>\n      <td>989.6</td>\n      <td>788.9</td>\n      <td>28</td>\n      <td>32.77</td>\n      <td>1778.5</td>\n      <td>...</td>\n      <td>0.032678</td>\n      <td>0.443576</td>\n      <td>0.194434</td>\n      <td>0.460093</td>\n      <td>0.539907</td>\n      <td>0.000000</td>\n      <td>0.507808</td>\n      <td>0.032678</td>\n      <td>11.178504</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>1029</th>\n      <td>260.9</td>\n      <td>100.5</td>\n      <td>78.3</td>\n      <td>200.6</td>\n      <td>8.6</td>\n      <td>864.5</td>\n      <td>761.5</td>\n      <td>28</td>\n      <td>32.40</td>\n      <td>1626.0</td>\n      <td>...</td>\n      <td>0.019559</td>\n      <td>0.468327</td>\n      <td>0.270418</td>\n      <td>0.593359</td>\n      <td>0.228565</td>\n      <td>0.178076</td>\n      <td>0.456220</td>\n      <td>0.019559</td>\n      <td>6.232273</td>\n      <td>784</td>\n    </tr>\n  </tbody>\n</table>\n<p>1030 rows × 23 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Extracting the Features & Splitting","metadata":{}},{"cell_type":"code","source":"#extract the explanatory and response variable\n#axis = 1 indicates that we are droping a column\n#axis = 1 indicates that we are droping from row\nX = df.drop('Strength', axis = 1)\nY = df['Strength']\n\n#split into train and test. use 80-20 rule\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.952467Z","iopub.execute_input":"2024-06-22T05:39:46.952748Z","iopub.status.idle":"2024-06-22T05:39:46.961202Z","shell.execute_reply.started":"2024-06-22T05:39:46.952724Z","shell.execute_reply":"2024-06-22T05:39:46.960364Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#instance\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\n\n#instance of the model and training\nmodel = SGDRegressor(loss='squared_error', alpha=0.00001, max_iter=1000, tol=1e-3, random_state=42, verbose = 1)\nmodel.fit(X_train_scaled, Y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.962518Z","iopub.execute_input":"2024-06-22T05:39:46.962892Z","iopub.status.idle":"2024-06-22T05:39:46.991993Z","shell.execute_reply.started":"2024-06-22T05:39:46.962861Z","shell.execute_reply":"2024-06-22T05:39:46.991054Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"-- Epoch 1\nNorm: 7.08, NNZs: 22, Bias: 31.480429, T: 824, Avg. loss: 184.616563\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 8.40, NNZs: 22, Bias: 34.656460, T: 1648, Avg. loss: 59.445908\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 9.96, NNZs: 22, Bias: 35.383297, T: 2472, Avg. loss: 53.014941\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 11.16, NNZs: 22, Bias: 35.767834, T: 3296, Avg. loss: 49.966974\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 12.36, NNZs: 22, Bias: 35.811245, T: 4120, Avg. loss: 47.967444\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 13.44, NNZs: 22, Bias: 35.698386, T: 4944, Avg. loss: 46.076160\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 14.48, NNZs: 22, Bias: 35.868651, T: 5768, Avg. loss: 44.741301\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 15.24, NNZs: 22, Bias: 35.777716, T: 6592, Avg. loss: 43.798907\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 16.00, NNZs: 22, Bias: 35.807683, T: 7416, Avg. loss: 42.820338\nTotal training time: 0.00 seconds.\n-- Epoch 10\nNorm: 16.77, NNZs: 22, Bias: 35.805277, T: 8240, Avg. loss: 42.057331\nTotal training time: 0.00 seconds.\n-- Epoch 11\nNorm: 17.46, NNZs: 22, Bias: 35.884158, T: 9064, Avg. loss: 41.229658\nTotal training time: 0.00 seconds.\n-- Epoch 12\nNorm: 18.02, NNZs: 22, Bias: 35.811434, T: 9888, Avg. loss: 40.814675\nTotal training time: 0.00 seconds.\n-- Epoch 13\nNorm: 18.59, NNZs: 22, Bias: 35.861667, T: 10712, Avg. loss: 40.213657\nTotal training time: 0.00 seconds.\n-- Epoch 14\nNorm: 19.08, NNZs: 22, Bias: 35.856638, T: 11536, Avg. loss: 39.821783\nTotal training time: 0.00 seconds.\n-- Epoch 15\nNorm: 19.55, NNZs: 22, Bias: 35.821122, T: 12360, Avg. loss: 39.407540\nTotal training time: 0.00 seconds.\n-- Epoch 16\nNorm: 19.98, NNZs: 22, Bias: 35.836511, T: 13184, Avg. loss: 39.015372\nTotal training time: 0.00 seconds.\n-- Epoch 17\nNorm: 20.42, NNZs: 22, Bias: 35.898185, T: 14008, Avg. loss: 38.721029\nTotal training time: 0.00 seconds.\n-- Epoch 18\nNorm: 20.84, NNZs: 22, Bias: 35.898895, T: 14832, Avg. loss: 38.431512\nTotal training time: 0.00 seconds.\n-- Epoch 19\nNorm: 21.19, NNZs: 22, Bias: 35.784300, T: 15656, Avg. loss: 38.255243\nTotal training time: 0.00 seconds.\n-- Epoch 20\nNorm: 21.58, NNZs: 22, Bias: 35.782422, T: 16480, Avg. loss: 38.103160\nTotal training time: 0.00 seconds.\n-- Epoch 21\nNorm: 21.90, NNZs: 22, Bias: 35.771492, T: 17304, Avg. loss: 37.980377\nTotal training time: 0.00 seconds.\n-- Epoch 22\nNorm: 22.21, NNZs: 22, Bias: 35.829257, T: 18128, Avg. loss: 37.714778\nTotal training time: 0.00 seconds.\n-- Epoch 23\nNorm: 22.49, NNZs: 22, Bias: 35.857871, T: 18952, Avg. loss: 37.551189\nTotal training time: 0.00 seconds.\n-- Epoch 24\nNorm: 22.75, NNZs: 22, Bias: 35.840778, T: 19776, Avg. loss: 37.295893\nTotal training time: 0.00 seconds.\n-- Epoch 25\nNorm: 22.99, NNZs: 22, Bias: 35.913169, T: 20600, Avg. loss: 37.375992\nTotal training time: 0.00 seconds.\n-- Epoch 26\nNorm: 23.27, NNZs: 22, Bias: 35.865120, T: 21424, Avg. loss: 37.249679\nTotal training time: 0.00 seconds.\n-- Epoch 27\nNorm: 23.47, NNZs: 22, Bias: 35.813750, T: 22248, Avg. loss: 37.115963\nTotal training time: 0.00 seconds.\n-- Epoch 28\nNorm: 23.69, NNZs: 22, Bias: 35.853967, T: 23072, Avg. loss: 37.115974\nTotal training time: 0.00 seconds.\n-- Epoch 29\nNorm: 23.91, NNZs: 22, Bias: 35.855791, T: 23896, Avg. loss: 36.996660\nTotal training time: 0.00 seconds.\n-- Epoch 30\nNorm: 24.05, NNZs: 22, Bias: 35.833618, T: 24720, Avg. loss: 36.731094\nTotal training time: 0.00 seconds.\n-- Epoch 31\nNorm: 24.29, NNZs: 22, Bias: 35.846693, T: 25544, Avg. loss: 36.923078\nTotal training time: 0.00 seconds.\n-- Epoch 32\nNorm: 24.43, NNZs: 22, Bias: 35.863104, T: 26368, Avg. loss: 36.760171\nTotal training time: 0.00 seconds.\n-- Epoch 33\nNorm: 24.63, NNZs: 22, Bias: 35.859926, T: 27192, Avg. loss: 36.740056\nTotal training time: 0.01 seconds.\n-- Epoch 34\nNorm: 24.77, NNZs: 22, Bias: 35.813386, T: 28016, Avg. loss: 36.673928\nTotal training time: 0.01 seconds.\n-- Epoch 35\nNorm: 24.90, NNZs: 22, Bias: 35.864076, T: 28840, Avg. loss: 36.694781\nTotal training time: 0.01 seconds.\n-- Epoch 36\nNorm: 25.05, NNZs: 22, Bias: 35.857226, T: 29664, Avg. loss: 36.689803\nTotal training time: 0.01 seconds.\n-- Epoch 37\nNorm: 25.17, NNZs: 22, Bias: 35.839534, T: 30488, Avg. loss: 36.620538\nTotal training time: 0.01 seconds.\n-- Epoch 38\nNorm: 25.33, NNZs: 22, Bias: 35.848650, T: 31312, Avg. loss: 36.432948\nTotal training time: 0.01 seconds.\n-- Epoch 39\nNorm: 25.41, NNZs: 22, Bias: 35.830959, T: 32136, Avg. loss: 36.519390\nTotal training time: 0.01 seconds.\n-- Epoch 40\nNorm: 25.55, NNZs: 22, Bias: 35.854908, T: 32960, Avg. loss: 36.494401\nTotal training time: 0.01 seconds.\n-- Epoch 41\nNorm: 25.63, NNZs: 22, Bias: 35.874331, T: 33784, Avg. loss: 36.458574\nTotal training time: 0.01 seconds.\n-- Epoch 42\nNorm: 25.77, NNZs: 22, Bias: 35.895571, T: 34608, Avg. loss: 36.400940\nTotal training time: 0.01 seconds.\n-- Epoch 43\nNorm: 25.83, NNZs: 22, Bias: 35.906763, T: 35432, Avg. loss: 36.429289\nTotal training time: 0.01 seconds.\n-- Epoch 44\nNorm: 25.91, NNZs: 22, Bias: 35.846990, T: 36256, Avg. loss: 36.356540\nTotal training time: 0.01 seconds.\n-- Epoch 45\nNorm: 26.01, NNZs: 22, Bias: 35.867165, T: 37080, Avg. loss: 36.353660\nTotal training time: 0.01 seconds.\n-- Epoch 46\nNorm: 26.09, NNZs: 22, Bias: 35.837631, T: 37904, Avg. loss: 36.304613\nTotal training time: 0.01 seconds.\n-- Epoch 47\nNorm: 26.16, NNZs: 22, Bias: 35.839333, T: 38728, Avg. loss: 36.291128\nTotal training time: 0.01 seconds.\n-- Epoch 48\nNorm: 26.26, NNZs: 22, Bias: 35.830600, T: 39552, Avg. loss: 36.379426\nTotal training time: 0.01 seconds.\n-- Epoch 49\nNorm: 26.37, NNZs: 22, Bias: 35.825559, T: 40376, Avg. loss: 36.265761\nTotal training time: 0.01 seconds.\n-- Epoch 50\nNorm: 26.42, NNZs: 22, Bias: 35.829775, T: 41200, Avg. loss: 36.237292\nTotal training time: 0.01 seconds.\n-- Epoch 51\nNorm: 26.50, NNZs: 22, Bias: 35.822775, T: 42024, Avg. loss: 36.186769\nTotal training time: 0.01 seconds.\n-- Epoch 52\nNorm: 26.58, NNZs: 22, Bias: 35.825132, T: 42848, Avg. loss: 36.172925\nTotal training time: 0.01 seconds.\n-- Epoch 53\nNorm: 26.62, NNZs: 22, Bias: 35.830566, T: 43672, Avg. loss: 36.285592\nTotal training time: 0.01 seconds.\n-- Epoch 54\nNorm: 26.71, NNZs: 22, Bias: 35.862600, T: 44496, Avg. loss: 36.160069\nTotal training time: 0.01 seconds.\n-- Epoch 55\nNorm: 26.75, NNZs: 22, Bias: 35.841301, T: 45320, Avg. loss: 36.226666\nTotal training time: 0.01 seconds.\n-- Epoch 56\nNorm: 26.80, NNZs: 22, Bias: 35.857185, T: 46144, Avg. loss: 36.194380\nTotal training time: 0.01 seconds.\n-- Epoch 57\nNorm: 26.86, NNZs: 22, Bias: 35.845898, T: 46968, Avg. loss: 36.166382\nTotal training time: 0.01 seconds.\n-- Epoch 58\nNorm: 26.90, NNZs: 22, Bias: 35.843849, T: 47792, Avg. loss: 36.201356\nTotal training time: 0.01 seconds.\n-- Epoch 59\nNorm: 26.99, NNZs: 22, Bias: 35.871621, T: 48616, Avg. loss: 35.976826\nTotal training time: 0.01 seconds.\n-- Epoch 60\nNorm: 26.98, NNZs: 22, Bias: 35.860049, T: 49440, Avg. loss: 36.135802\nTotal training time: 0.01 seconds.\n-- Epoch 61\nNorm: 27.06, NNZs: 22, Bias: 35.855643, T: 50264, Avg. loss: 36.093298\nTotal training time: 0.01 seconds.\n-- Epoch 62\nNorm: 27.09, NNZs: 22, Bias: 35.844644, T: 51088, Avg. loss: 36.151049\nTotal training time: 0.01 seconds.\n-- Epoch 63\nNorm: 27.13, NNZs: 22, Bias: 35.863040, T: 51912, Avg. loss: 36.152954\nTotal training time: 0.01 seconds.\n-- Epoch 64\nNorm: 27.16, NNZs: 22, Bias: 35.851678, T: 52736, Avg. loss: 36.049060\nTotal training time: 0.01 seconds.\nConvergence after 64 epochs took 0.01 seconds\n","output_type":"stream"},{"execution_count":149,"output_type":"execute_result","data":{"text/plain":"SGDRegressor(alpha=1e-05, random_state=42, verbose=1)","text/html":"<style>#sk-container-id-24 {color: black;background-color: white;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(alpha=1e-05, random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(alpha=1e-05, random_state=42, verbose=1)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#evaluating the model's performance\nY_pred = model.predict(X_test_scaled)\nmse = mean_squared_error(Y_test, Y_pred)\nprint(f\"Mean squared error on testing set: {mse}\")\n\nmpe = np.mean((Y_pred - Y_test) / Y_test) * 100\nprint(f\"Mean percentage error on testing set: {mpe} %\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:46.992992Z","iopub.execute_input":"2024-06-22T05:39:46.993253Z","iopub.status.idle":"2024-06-22T05:39:46.999552Z","shell.execute_reply.started":"2024-06-22T05:39:46.993231Z","shell.execute_reply":"2024-06-22T05:39:46.998614Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"Mean squared error on testing set: 63.07437383378309\nMean percentage error on testing set: 9.459451079616626 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Observe that the cost function is reduced. The prior is 97.480, and after including the engineered features the cost function is reduced to 63.67. However, it look longer for the model to converge. This is expected since there are more features. ","metadata":{}},{"cell_type":"markdown","source":"## Polynomial Regression using LinearRegression() class\nThis class implements linear regression models, from simple linear regression to multiple and polynomial regression. This uses the Ordinary Least Squares (OLS) algorithm to fit a linear regression model. This algorithm finds the coefficients of the linear equation that minimizes the squared differences betweent the observed and predicted values of the target variable. ","metadata":{}},{"cell_type":"markdown","source":"### Generating Polynomial Features using PolynomialFeature Class from scikitlearn.preprocessing\n\nPolynomialFeature class generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].","metadata":{}},{"cell_type":"code","source":"#extract the explanatory and response variable\n#axis = 1 indicates that we are droping a column\n#axis = 1 indicates that we are droping from row\nX = df2.drop('Strength', axis = 1)\nY = df2['Strength']\n\n#split into train and test. use 80-20 rule\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.000669Z","iopub.execute_input":"2024-06-22T05:39:47.000956Z","iopub.status.idle":"2024-06-22T05:39:47.009516Z","shell.execute_reply.started":"2024-06-22T05:39:47.000929Z","shell.execute_reply":"2024-06-22T05:39:47.008749Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\n#instance \npoly_features = PolynomialFeatures(degree = 2, include_bias = False)\n\n\n\n#create poly features for X_trian and X_test\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.010583Z","iopub.execute_input":"2024-06-22T05:39:47.010859Z","iopub.status.idle":"2024-06-22T05:39:47.022856Z","shell.execute_reply.started":"2024-06-22T05:39:47.010835Z","shell.execute_reply":"2024-06-22T05:39:47.022009Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"\n# Create and fit the model\nmodel = LinearRegression()\nmodel.fit(X_train_poly, Y_train)\n\n# Make predictions\nY_test_pred = model.predict(X_test_poly)\n\n\n#evaluating the model's performance\nY_pred = model.predict(X_test_poly)\nmse = mean_squared_error(Y_test, Y_pred)\nprint(f\"Mean squared error on testing set: {mse}\")\n\nmpe = np.mean((Y_pred - Y_test) / Y_test) * 100\nprint(f\"Mean percentage error on testing set: {mpe} %\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.109359Z","iopub.execute_input":"2024-06-22T05:39:47.109660Z","iopub.status.idle":"2024-06-22T05:39:47.124381Z","shell.execute_reply.started":"2024-06-22T05:39:47.109635Z","shell.execute_reply":"2024-06-22T05:39:47.123137Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stdout","text":"Mean squared error on testing set: 55.582457881024105\nMean percentage error on testing set: 5.535528217238074 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**By including the polynomial terms and interaction terms in our model, \nthe mpe is reduced from 9.45 (This include engineered features based on material science) % to 1.14% (while this include all combination, i.e., All polynomial combination\nof features with degree less than or euqal to the specified degree).** ","metadata":{}},{"cell_type":"markdown","source":"Using the original features to create new polynomial features and interaction features using PolynomialFeature class, we can see a significant reduction to the cost function (MSE = 55.582). This model is better compared to the preceeding models. ","metadata":{}},{"cell_type":"markdown","source":"## Regularized Linear Regression\nRegularized regression is a technique used in machine learning to improve the performance and generalizability of regression models by adding a penalty term to the loss function. This penalty discourages complex models that are likely to overfit the training data. Regularization methods adjust the model to be simpler, often resulting in better performance on unseen data.\n\nTypes include:\n*     -Ridge Regression (L2 Regularization)\n*     -Lasso Regression (L1 Regulzarization)\n","metadata":{}},{"cell_type":"markdown","source":"### L1 Regularization","metadata":{}},{"cell_type":"code","source":"#extract the explanatory and response variable\n#axis = 1 indicates that we are droping a column\n#axis = 1 indicates that we are droping from row\nX = df3.drop('Strength', axis = 1)\nY = df3['Strength']\n\n#split into train and test. use 80-20 rule\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.126545Z","iopub.execute_input":"2024-06-22T05:39:47.127942Z","iopub.status.idle":"2024-06-22T05:39:47.142831Z","shell.execute_reply.started":"2024-06-22T05:39:47.127908Z","shell.execute_reply":"2024-06-22T05:39:47.141702Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"df3","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.148549Z","iopub.execute_input":"2024-06-22T05:39:47.148993Z","iopub.status.idle":"2024-06-22T05:39:47.181667Z","shell.execute_reply.started":"2024-06-22T05:39:47.148959Z","shell.execute_reply":"2024-06-22T05:39:47.180688Z"},"trusted":true},"execution_count":155,"outputs":[{"execution_count":155,"output_type":"execute_result","data":{"text/plain":"      Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0      540.0                 0.0      0.0  162.0               2.5   \n1      540.0                 0.0      0.0  162.0               2.5   \n2      332.5               142.5      0.0  228.0               0.0   \n3      332.5               142.5      0.0  228.0               0.0   \n4      198.6               132.4      0.0  192.0               0.0   \n...      ...                 ...      ...    ...               ...   \n1025   276.4               116.0     90.3  179.6               8.9   \n1026   322.2                 0.0    115.6  196.0              10.4   \n1027   148.5               139.4    108.6  192.7               6.1   \n1028   159.1               186.7      0.0  175.6              11.3   \n1029   260.9               100.5     78.3  200.6               8.6   \n\n      Coarse Aggregate  Fine Aggregate  Age  Strength  \n0               1040.0           676.0   28     79.99  \n1               1055.0           676.0   28     61.89  \n2                932.0           594.0  270     40.27  \n3                932.0           594.0  365     41.05  \n4                978.4           825.5  360     44.30  \n...                ...             ...  ...       ...  \n1025             870.1           768.3   28     44.28  \n1026             817.9           813.4   28     31.18  \n1027             892.4           780.0   28     23.70  \n1028             989.6           788.9   28     32.77  \n1029             864.5           761.5   28     32.40  \n\n[1030 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1025</th>\n      <td>276.4</td>\n      <td>116.0</td>\n      <td>90.3</td>\n      <td>179.6</td>\n      <td>8.9</td>\n      <td>870.1</td>\n      <td>768.3</td>\n      <td>28</td>\n      <td>44.28</td>\n    </tr>\n    <tr>\n      <th>1026</th>\n      <td>322.2</td>\n      <td>0.0</td>\n      <td>115.6</td>\n      <td>196.0</td>\n      <td>10.4</td>\n      <td>817.9</td>\n      <td>813.4</td>\n      <td>28</td>\n      <td>31.18</td>\n    </tr>\n    <tr>\n      <th>1027</th>\n      <td>148.5</td>\n      <td>139.4</td>\n      <td>108.6</td>\n      <td>192.7</td>\n      <td>6.1</td>\n      <td>892.4</td>\n      <td>780.0</td>\n      <td>28</td>\n      <td>23.70</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>159.1</td>\n      <td>186.7</td>\n      <td>0.0</td>\n      <td>175.6</td>\n      <td>11.3</td>\n      <td>989.6</td>\n      <td>788.9</td>\n      <td>28</td>\n      <td>32.77</td>\n    </tr>\n    <tr>\n      <th>1029</th>\n      <td>260.9</td>\n      <td>100.5</td>\n      <td>78.3</td>\n      <td>200.6</td>\n      <td>8.6</td>\n      <td>864.5</td>\n      <td>761.5</td>\n      <td>28</td>\n      <td>32.40</td>\n    </tr>\n  </tbody>\n</table>\n<p>1030 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n\n# #instance\n# scaler = StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_test_scaled = scaler.fit_transform(X_test)\n\n# #instance of the model and training\n# model = SGDRegressor(loss='squared_error', alpha=0.00001, max_iter=1000, tol=1e-3, random_state=42, verbose = 1)\n# model.fit(X_train_scaled, Y_train)\n\n\n#instance of the model and training\nmodel = Lasso(alpha = 0.1, random_state = 42)\nmodel.fit(X_train, Y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.186137Z","iopub.execute_input":"2024-06-22T05:39:47.188613Z","iopub.status.idle":"2024-06-22T05:39:47.207187Z","shell.execute_reply.started":"2024-06-22T05:39:47.188575Z","shell.execute_reply":"2024-06-22T05:39:47.206249Z"},"trusted":true},"execution_count":156,"outputs":[{"execution_count":156,"output_type":"execute_result","data":{"text/plain":"Lasso(alpha=0.1, random_state=42)","text/html":"<style>#sk-container-id-25 {color: black;background-color: white;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso(alpha=0.1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(alpha=0.1, random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"#evaluating the model's performance\nY_pred = model.predict(X_test)\nmse = mean_squared_error(Y_test, Y_pred)\nprint(f\"Mean squared error on testing set: {mse}\")\n\nmpe = np.mean((Y_pred - Y_test) / Y_test) * 100\nprint(f\"Mean percentage error on testing set: {mpe} %\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.213466Z","iopub.execute_input":"2024-06-22T05:39:47.215907Z","iopub.status.idle":"2024-06-22T05:39:47.229480Z","shell.execute_reply.started":"2024-06-22T05:39:47.215869Z","shell.execute_reply":"2024-06-22T05:39:47.228374Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"Mean squared error on testing set: 95.9651693895091\nMean percentage error on testing set: 12.825271750806857 %\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#instance\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\n\n\n\n#instance of the model and training\nmodel = Lasso(alpha = 0.1, random_state = 42)\nmodel.fit(X_train_scaled, Y_train)\n\n\n#evaluating the model's performance\nY_pred = model.predict(X_test_scaled)\nmse = mean_squared_error(Y_test, Y_pred)\nprint(f\"Mean squared error on testing set: {mse}\")\n\nmpe = np.mean((Y_pred - Y_test) / Y_test) * 100\nprint(f\"Mean percentage error on testing set: {mpe} %\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.234140Z","iopub.execute_input":"2024-06-22T05:39:47.236607Z","iopub.status.idle":"2024-06-22T05:39:47.252593Z","shell.execute_reply.started":"2024-06-22T05:39:47.236569Z","shell.execute_reply":"2024-06-22T05:39:47.251707Z"},"trusted":true},"execution_count":158,"outputs":[{"name":"stdout","text":"Mean squared error on testing set: 98.09725887178499\nMean percentage error on testing set: 13.583652406728438 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### L2 Regularization\n","metadata":{}},{"cell_type":"code","source":"#extract the explanatory and response variable\n#axis = 1 indicates that we are droping a column\n#axis = 1 indicates that we are droping from row\nX = df3.drop('Strength', axis = 1)\nY = df3['Strength']\n\n#split into train and test. use 80-20 rule\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.253587Z","iopub.execute_input":"2024-06-22T05:39:47.253854Z","iopub.status.idle":"2024-06-22T05:39:47.260998Z","shell.execute_reply.started":"2024-06-22T05:39:47.253831Z","shell.execute_reply":"2024-06-22T05:39:47.259995Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n\n# #instance\n# scaler = StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_test_scaled = scaler.fit_transform(X_test)\n\n# #instance of the model and training\n# model = SGDRegressor(loss='squared_error', alpha=0.00001, max_iter=1000, tol=1e-3, random_state=42, verbose = 1)\n# model.fit(X_train_scaled, Y_train)\n\n\n#instance of the model and training\nmodel = Ridge(alpha = 1, random_state = 42)\nmodel.fit(X_train, Y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.262071Z","iopub.execute_input":"2024-06-22T05:39:47.262375Z","iopub.status.idle":"2024-06-22T05:39:47.274859Z","shell.execute_reply.started":"2024-06-22T05:39:47.262349Z","shell.execute_reply":"2024-06-22T05:39:47.273934Z"},"trusted":true},"execution_count":160,"outputs":[{"execution_count":160,"output_type":"execute_result","data":{"text/plain":"Ridge(alpha=1, random_state=42)","text/html":"<style>#sk-container-id-26 {color: black;background-color: white;}#sk-container-id-26 pre{padding: 0;}#sk-container-id-26 div.sk-toggleable {background-color: white;}#sk-container-id-26 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-26 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-26 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-26 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-26 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-26 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-26 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-26 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-26 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-26 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-26 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-26 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-26 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-26 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-26 div.sk-item {position: relative;z-index: 1;}#sk-container-id-26 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-26 div.sk-item::before, #sk-container-id-26 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-26 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-26 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-26 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-26 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-26 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-26 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-26 div.sk-label-container {text-align: center;}#sk-container-id-26 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-26 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-26\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" checked><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=1, random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"#evaluating the model's performance\nY_pred = model.predict(X_test)\nmse = mean_squared_error(Y_test, Y_pred)\nprint(f\"Mean squared error on testing set: {mse}\")\n\nmpe = np.mean((Y_pred - Y_test) / Y_test) * 100\nprint(f\"Mean percentage error on testing set: {mpe} %\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.276180Z","iopub.execute_input":"2024-06-22T05:39:47.276488Z","iopub.status.idle":"2024-06-22T05:39:47.287576Z","shell.execute_reply.started":"2024-06-22T05:39:47.276457Z","shell.execute_reply":"2024-06-22T05:39:47.286585Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"Mean squared error on testing set: 95.97089554036008\nMean percentage error on testing set: 12.805407824372592 %\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#instance\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\n\n\n#instance of the model and training\nmodel = Lasso(alpha = 0.1, random_state = 42)\nmodel.fit(X_train_scaled, Y_train)\n\n\n#evaluating the model's performance\nY_pred = model.predict(X_test_scaled)\nmse = mean_squared_error(Y_test, Y_pred)\nprint(f\"Mean squared error on testing set: {mse}\")\n\nmpe = np.mean((Y_pred - Y_test) / Y_test) * 100\nprint(f\"Mean percentage error on testing set: {mpe} %\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.288670Z","iopub.execute_input":"2024-06-22T05:39:47.288956Z","iopub.status.idle":"2024-06-22T05:39:47.305054Z","shell.execute_reply.started":"2024-06-22T05:39:47.288933Z","shell.execute_reply":"2024-06-22T05:39:47.304265Z"},"trusted":true},"execution_count":162,"outputs":[{"name":"stdout","text":"Mean squared error on testing set: 98.09725887178499\nMean percentage error on testing set: 13.583652406728438 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### L2 regularization using Gradeint Descent Algorithm\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Define your compute_gradient_linear_reg function here\n\ndef compute_gradient_linear_reg(X, y, w, b, lambda_):\n    \"\"\"\n    Computes the gradient for linear regression with L2 regularization\n    \n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n      \n    Returns:\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m, n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]                 \n        for j in range(n):                         \n            dj_dw[j] += err * X[i, j]               \n        dj_db += err                        \n    dj_dw /= m                                \n    dj_db /= m   \n    \n    for j in range(n):\n        dj_dw[j] += (lambda_ / m) * w[j]\n\n    return dj_db, dj_dw\n\n\n# # Assume df3 is your dataframe\n# df3 = pd.DataFrame({\n#     'Feature1': np.random.rand(100),\n#     'Feature2': np.random.rand(100),\n#     'Feature3': np.random.rand(100),\n#     'Strength': np.random.rand(100)\n# })\n\n# Separate X and Y\nX = df3.drop('Strength', axis=1).values  # Convert to numpy array\nY = df3['Strength'].values  # Convert to numpy array\n\n# Split into train and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Example usage of compute_gradient_linear_reg function\nnp.random.seed(1)\nw_tmp = np.random.rand(X_train.shape[1])  # Initialize weights\nb_tmp = 0.5  # Initialize bias\nlambda_tmp = 1  # Regularization parameter\n\n# Compute gradient\ndj_db_tmp, dj_dw_tmp = compute_gradient_linear_reg(X_train, Y_train, w_tmp, b_tmp, lambda_tmp)\n\n# Print results\nprint(f\"dj_db: {dj_db_tmp}\")\nprint(f\"Regularized dj_dw:\\n{dj_dw_tmp.tolist()}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.306232Z","iopub.execute_input":"2024-06-22T05:39:47.306536Z","iopub.status.idle":"2024-06-22T05:39:47.329747Z","shell.execute_reply.started":"2024-06-22T05:39:47.306512Z","shell.execute_reply":"2024-06-22T05:39:47.328734Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"dj_db: 441.3975910478721\nRegularized dj_dw:\n[126754.37123016847, 36353.3900446637, 20968.843446967097, 80468.21049186478, 2692.4045960339795, 428422.5208231668, 339548.4656900214, 20662.484731241344]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef compute_gradient_linear_reg(X, y, w, b, lambda_):\n    \"\"\"\n    Computes the gradient for linear regression with L2 regularization\n    \n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n      \n    Returns:\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n      mse (float):          Mean Squared Error for current parameters\n    \"\"\"\n    m, n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n    mse = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]                 \n        for j in range(n):                         \n            dj_dw[j] += err * X[i, j]               \n        dj_db += err\n        \n        # Compute MSE\n        mse += err ** 2\n\n    dj_dw /= m                                \n    dj_db /= m\n    mse /= m\n    \n    # Add regularization to gradients\n    for j in range(n):\n        dj_dw[j] += (lambda_ / m) * w[j]\n\n    return dj_db, dj_dw, mse\n\n\n\n# Separate X and Y\nX = df3.drop('Strength', axis=1).values  # Convert to numpy array\nY = df3['Strength'].values  # Convert to numpy array\n\n# Split into train and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Example usage of compute_gradient_linear_reg function\nnp.random.seed(1)\nw_tmp = np.random.rand(X_train.shape[1])  # Initialize weights\nb_tmp = 0.5  # Initialize bias\nlambda_tmp = 0.7  # Regularization parameter\nlearning_rate = 0.01  # Learning rate for gradient descent\nn_iterations = 1000  # Number of iterations\n\n# Perform gradient descent\nfor iteration in range(n_iterations):\n    dj_db_tmp, dj_dw_tmp, mse_tmp = compute_gradient_linear_reg(X_train, Y_train, w_tmp, b_tmp, lambda_tmp)\n    \n    # Update weights and bias using gradient descent\n    w_tmp -= learning_rate * dj_dw_tmp\n    b_tmp -= learning_rate * dj_db_tmp\n    \n    # Display MSE for each iteration\n    if iteration % 100 == 0:\n        print(f\"Iteration {iteration}: MSE = {mse_tmp}\")\n\n# Optionally, print final weights and bias\nprint(\"\\nFinal weights:\")\nprint(w_tmp)\nprint(\"Final bias:\")\nprint(b_tmp)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T05:39:47.330941Z","iopub.execute_input":"2024-06-22T05:39:47.331246Z","iopub.status.idle":"2024-06-22T05:39:51.912678Z","shell.execute_reply.started":"2024-06-22T05:39:47.331215Z","shell.execute_reply":"2024-06-22T05:39:51.911702Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stdout","text":"Iteration 0: MSE = 197824.06836992712\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/3513319929.py:33: RuntimeWarning: overflow encountered in scalar power\n  mse += err ** 2\n/tmp/ipykernel_33/3513319929.py:29: RuntimeWarning: overflow encountered in scalar add\n  dj_dw[j] += err * X[i, j]\n/tmp/ipykernel_33/3513319929.py:29: RuntimeWarning: invalid value encountered in scalar multiply\n  dj_dw[j] += err * X[i, j]\n/tmp/ipykernel_33/3513319929.py:67: RuntimeWarning: invalid value encountered in subtract\n  w_tmp -= learning_rate * dj_dw_tmp\n","output_type":"stream"},{"name":"stdout","text":"Iteration 100: MSE = nan\nIteration 200: MSE = nan\nIteration 300: MSE = nan\nIteration 400: MSE = nan\nIteration 500: MSE = nan\nIteration 600: MSE = nan\nIteration 700: MSE = nan\nIteration 800: MSE = nan\nIteration 900: MSE = nan\n\nFinal weights:\n[nan nan nan nan nan nan nan nan]\nFinal bias:\nnan\n","output_type":"stream"}]}]}